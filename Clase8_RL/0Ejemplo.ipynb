{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "difficult-regulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "whole-collect",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Environment class\n",
    "class MyEnvironment:\n",
    "\n",
    "  def __init__(self):\n",
    "    #maximum number of steps which the agent can take to gain rewards\n",
    "    self.remaining_steps=20 #assume that the game must be completed within 20 steps\n",
    "\n",
    "  def get_observation(self):\n",
    "    #it can be any number of coordinates.Its considered as 3 here.\n",
    "    #These values -0.0,0.0,0.0 represent some kind of logic that gives info about the environment.These values can be anything.\n",
    "    return [1.0,2.0,1.0]  \n",
    "  \n",
    "  #when agent,performs an action,it should get a reward\n",
    "  #i have set it as 1 for reward,-1 for punishment\n",
    "  def get_actions(self):\n",
    "    return [-1,1]\n",
    "\n",
    "  #if steps are completed,return True because the agent should not move anymore\n",
    "  def check_is_done(self)->bool:\n",
    "    return self.remaining_steps==0\n",
    "\n",
    "  def action(self,int):\n",
    "    if self.check_is_done():\n",
    "      raise Exception(\"Game over\")      \n",
    "    self.remaining_steps-=1  #if steps can still be taken-game not finished=>decrement the remaining number of steps\n",
    "    return random.random()  #here-as this is a simple implementation-just returning a random number\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dedicated-meaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent implements some policy\n",
    "\n",
    "class myAgent:\n",
    "  def __init__(self):\n",
    "    self.total_rewards=0.0 #initially-agent-no rewards\n",
    "\n",
    "  def step(self,ob:MyEnvironment):\n",
    "    curr_obs=ob.get_observation()\n",
    "    print(curr_obs)\n",
    "    curr_action=ob.get_actions()\n",
    "    print(curr_action)\n",
    "    curr_reward=ob.action(random.choice(curr_action)) \n",
    "    #here,we are randomly picking -1 or 1\n",
    "    #usually,when action() is invoked,implementation to check if the decision of the agent is crt-give positive reward else negative reward\n",
    "    self.total_rewards+=curr_reward\n",
    "    print(\"Total rewards so far= %.3f \"%self.total_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-white",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "prostate-little",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 0.918 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 1.520 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 1.573 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 1.656 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 2.278 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 2.642 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 3.269 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 3.655 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 4.304 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 4.843 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 5.421 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 6.084 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 6.161 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 6.587 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 7.494 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 7.754 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 8.385 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 9.175 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 9.602 \n",
      "[1.0, 2.0, 1.0]\n",
      "[-1, 1]\n",
      "Total rewards so far= 10.054 \n",
      "Total reward is 10.054 \n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "  obj=MyEnvironment()\n",
    "  agent=myAgent()\n",
    "  step_number=0\n",
    "\n",
    "  while not obj.check_is_done():\n",
    "    step_number+=1\n",
    "    agent.step(obj)\n",
    "    \n",
    "\n",
    "  print(\"Total reward is %.3f \"%agent.total_rewards)\n",
    "  #different o/p everytime we run this code b'coz diff random numbers will be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-front",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
