{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='device=cpu'\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "experiment_setup_name = \"tutorial.gym.atari.spaceinvaders-v0.cnn\"\n",
    "\n",
    "\n",
    "#gym game title\n",
    "GAME_TITLE = 'SpaceInvaders-v0'\n",
    "\n",
    "#how many parallel game instances can your machine tolerate\n",
    "N_PARALLEL_GAMES = 5\n",
    "\n",
    "#how long is one replay session from a batch\n",
    "#since we have window-like memory (no recurrent layers), we can use relatively small session weights\n",
    "replay_seq_len = 10\n",
    "\n",
    "\n",
    "#theano device selection. GPU is, as always, in preference, but not required\n",
    "%env THEANO_FLAGS='device=cpu'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This tutorial is a showcase on how to use AgentNet for OpenAI Gym environments\n",
    "\n",
    "\n",
    "* Space Invaders game as an example\n",
    "* Training a simple lasagne neural network for Q_learning objective\n",
    " * This example can be easily modified to use convolutional networks and/or recurrent agent memory. \n",
    " \n",
    "* Training via simple experience replay (explained below)\n",
    "* Only using utility recurrent layers for simplicity of this example\n",
    " * but adding a few RNNs or GRUs shouldn's be a problem\n",
    "* the network is trained with a simple one-step Q-learning for simplicity\n",
    "\n",
    "\n",
    "## About OpenAI Gym\n",
    "\n",
    "* Its a recently published platform that basicly allows you to train agents in a wide variety of environments with near-identical interface.\n",
    "* This is twice as awesome since now we don't need to write a new wrapper for every game\n",
    "* Go check it out!\n",
    "  * Blog post - https://openai.com/blog/openai-gym-beta/\n",
    "  * Github - https://github.com/openai/gym\n",
    "  \n",
    "### Installing it\n",
    " * If nothing changed on their side, to run this, you bacically need to follow their install instructions - \n",
    " \n",
    "```\n",
    "git clone https://github.com/openai/gym.git\n",
    "cd gym\n",
    "pip install -e .[all]\n",
    "```\n",
    "\n",
    "## New to AgentNet and Lasagne?\n",
    "* This is pretty much the basic tutorial for AgentNet, so it's okay not to know it.\n",
    "* We only require surface level knowledge of theano and lasagne, so you can just learn them as you go.\n",
    "* Alternatively, you can find Lasagne tutorials here:\n",
    " * Official mnist example: http://lasagne.readthedocs.io/en/latest/user/tutorial.html\n",
    " * From scratch: https://github.com/ddtm/dl-course/tree/master/Seminar4\n",
    " * From theano: https://github.com/craffel/Lasagne-tutorial/blob/master/examples/tutorial.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment setup\n",
    "* Here we basically just load the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2958d24d250>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "atari = gym.make(GAME_TITLE)\n",
    "atari.reset()\n",
    "plt.imshow(atari.render('rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Parameters\n",
    "* observation dimensions, actions, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "n_actions = atari.action_space.n\n",
    "observation_shape = (None,)+atari.observation_space.shape\n",
    "action_names = atari.get_action_meanings()\n",
    "print(action_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "del atari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# agent setup step by step\n",
    "* An agent implementation may contain these parts:\n",
    " * Observation(s)\n",
    "   * InputLayers where observed game states (here - images) are sent at each tick \n",
    " * Memory layer(s)\n",
    "   * A dictionary that maps \"New memory layers\" to \"prev memory layers\"\n",
    " * Policy layer (e.g. Q-values or probabilities)\n",
    "   * in this case, a lasagne dense layer based on observation layer\n",
    " * Resolver - acton picker layer\n",
    "   * chooses what action to take given Q-values\n",
    "   * in this case, the resolver has epsilon-greedy policy\n",
    "   \n",
    "   \n",
    "We are going to build something of this shape:\n",
    "\n",
    "(one can assume that the 'time' goes from left to right, inputs are at the bottom and outputs go to the top)\n",
    "\n",
    "\n",
    "\n",
    "![window_dqn_scheme](http://s32.postimg.org/yy5q3wadx/window_dqn.png)\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Agent observations\n",
    "\n",
    "* Here you define where observations (game images) appear in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import InputLayer, DimshuffleLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#image observation at current tick goes here\n",
    "observation_layer = InputLayer(observation_shape,\n",
    "                               name=\"images input\")\n",
    "\n",
    "\n",
    "#reshape to [batch, color, x, y] to allow for convolutional layers to work correctly\n",
    "observation_reshape = DimshuffleLayer(observation_layer,(0,3,1,2))\n",
    "\n",
    "observation_reshape = lasagne.layers.Pool2DLayer(observation_reshape,(2,2),mode='average_inc_pad')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "### Agent memory states\n",
    " * Here you can define arbitrary transitions between \"previous state\" variables and their next states\n",
    " * The rules are\n",
    "   * previous states must be input layers\n",
    "   * next states must have same shape as previous ones\n",
    "   * otherwise it can be any lasagne network\n",
    "   * AgentNet.memory has several useful layers\n",
    "   \n",
    " * During training and evaluation, your states will be updated recurrently\n",
    "   * next state at t=1 is given as previous state to t=2\n",
    " \n",
    " * Finally, you have to define a dictionary mapping new state -> previous state\n",
    "\n",
    "\n",
    "### In this demo\n",
    "Since we have almost fully observable environment AND we want to keep baseline simple, we shall use no recurrent units.\n",
    "However, Atari game environments are known to have __flickering__ effect where some sprites are shown only on odd frames and others on even ones - that was used to optimize performance at the time.\n",
    "To compensate for this, we shall use the memory layer called __WindowAugmentation__ which basically maintains a K previous time steps of what it is fed with.\n",
    "\n",
    "One can try to use\n",
    " * GRU - `from agentnet.memory import GRUMemoryLayer`\n",
    " * RNN - `from agentnet.memory import RNNCell`\n",
    " * any custom lasagne layers that compute new memory states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#memory\n",
    "#using simple window-based memory that stores several states\n",
    "#the SpaceInvaders environment does not need any more as it is almost fully-observed\n",
    "from agentnet.memory import WindowAugmentation\n",
    "\n",
    "\n",
    "window_size = 3\n",
    "\n",
    "\n",
    "#prev state input\n",
    "prev_window = InputLayer((None,window_size)+tuple(observation_reshape.output_shape[1:]),\n",
    "                        name = \"previous window state\")\n",
    "\n",
    "\n",
    "#our window\n",
    "window = WindowAugmentation(observation_reshape,\n",
    "                            prev_window,\n",
    "                            name = \"new window state\")\n",
    "\n",
    "\n",
    "\n",
    "memory_dict = {window:prev_window}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neural network body\n",
    "Our strategy, again:\n",
    " * take pixel-wise maximum over the window\n",
    " * apply some layers\n",
    " * use output layer to predict Q-values(see next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import DropoutLayer,DenseLayer, ExpressionLayer\n",
    "#you may use any other lasagne layers, including convolutions, batch_norms, maxout, etc\n",
    "\n",
    "#pixel-wise maximum over the temporal window (to avoid flickering)\n",
    "window_max = ExpressionLayer(window,\n",
    "                             lambda a: a.max(axis=1),\n",
    "                             output_shape = (None,)+window.output_shape[2:])\n",
    "\n",
    "\n",
    "\n",
    "#a simple lasagne network (try replacing with any other lasagne network and see what works best)    \n",
    "nn = lasagne.layers.Conv2DLayer(window_max,32,filter_size=8,stride=(4,4), name='cnn0')\n",
    "nn = lasagne.layers.Conv2DLayer(nn,64,filter_size=8,stride=(4,4), name='cnn1 ыть')\n",
    "\n",
    "#nn = DropoutLayer(nn,name = \"dropout\", p=0.05) #will get deterministic during evaluation\n",
    "nn = DenseLayer(nn,num_units=256,name='dense1')\n",
    "\n",
    "\n",
    "#WARNING! if your network is computing too slowly, try decreasing the amount of neurons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Agent policy and action picking\n",
    "* Since we are training a deep Q-network, we need it to predict Q-values and take actions.\n",
    "* Hence we define a lasagne layer that is used for action output\n",
    "\n",
    "* To pick actions, we use an epsilon-greedy resolver\n",
    "  * Note that resolver outputs particular action IDs and not probabilities.\n",
    "  * These actions are than sent into the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#q_eval\n",
    "q_eval = DenseLayer(nn,\n",
    "                   num_units = n_actions,\n",
    "                   nonlinearity=lasagne.nonlinearities.linear,\n",
    "                   name=\"QEvaluator\")\n",
    "\n",
    "#resolver\n",
    "from agentnet.resolver import EpsilonGreedyResolver\n",
    "resolver = EpsilonGreedyResolver(q_eval,name=\"resolver\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally, agent\n",
    "We declare that this network is and MDP agent with such and such inputs, states and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentnet.agent import Agent\n",
    "#all together\n",
    "agent = Agent(observation_layer,\n",
    "              memory_dict,\n",
    "              q_eval,resolver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[cnn0.W,\n",
       " cnn0.b,\n",
       " cnn1 ыть.W,\n",
       " cnn1 ыть.b,\n",
       " dense1.W,\n",
       " dense1.b,\n",
       " QEvaluator.W,\n",
       " QEvaluator.b]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights, output, etc\n",
    "weights = lasagne.layers.get_all_params(resolver,trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent step function\n",
    "* computes action and next state given observation and prev state\n",
    "* written in a generic way to support any recurrences, windows, LTMs, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#compile theano graph for one step decision making\n",
    "applier_fun = agent.get_react_function()\n",
    "\n",
    "#a nice pythonic interface\n",
    "def step(observation, prev_memories = 'zeros',batch_size = N_PARALLEL_GAMES):\n",
    "    \"\"\" returns actions and new states given observation and prev state\n",
    "    Prev state in default setup should be [prev window,]\"\"\"\n",
    "    #default to zeros\n",
    "    if prev_memories == 'zeros':\n",
    "        prev_memories = [np.zeros((batch_size,)+tuple(mem.output_shape[1:]),\n",
    "                                  dtype='float32') \n",
    "                         for mem in agent.agent_states]\n",
    "    res = applier_fun(np.array(observation),*prev_memories)\n",
    "    action = res[0]\n",
    "    memories = res[1:]\n",
    "    return action,memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and manage a pool of atari sessions to play with\n",
    "\n",
    "* To make training more stable, we shall have an entire batch of game sessions each happening independent of others\n",
    "* We define a small container that stores\n",
    " * game emulators\n",
    " * last agent observations\n",
    " * agent memories at last time tick\n",
    "* This allows us to instantly continue a session from where it stopped\n",
    "\n",
    "\n",
    "\n",
    "* Why several parallel agents help training: http://arxiv.org/pdf/1602.01783v1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'atari' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-cfdb34823f1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0magentnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperiments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopenai_gym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEnvPool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEnvPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGAME_TITLE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0matari\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_PARALLEL_GAMES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'atari' is not defined"
     ]
    }
   ],
   "source": [
    "from agentnet.experiments.openai_gym.pool import EnvPool\n",
    "\n",
    "pool = EnvPool(GAME_TITLE,atari, N_PARALLEL_GAMES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "observation_log,action_log,reward_log,_,_,_  = pool.interact(step,50)\n",
    "\n",
    "print(np.array(action_names)[np.array(action_log)[:3,:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experience replay pool\n",
    "\n",
    "Since our network exists in a theano graph and OpenAI gym doesn't, we shall train out network via experience replay.\n",
    "\n",
    "To do that in AgentNet, one can use a SessionPoolEnvironment.\n",
    "\n",
    "It's simple: you record new sessions using `interact(...)`, and than immediately train on them.\n",
    "\n",
    "1. Interact with Atari, get play sessions\n",
    "2. Store them into session environment\n",
    "3. Train on them\n",
    "4. Repeat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Create an environment with all default parameters\n",
    "from agentnet.environment import SessionPoolEnvironment\n",
    "env = SessionPoolEnvironment(observations = observation_layer,\n",
    "                             actions=resolver,\n",
    "                             agent_memories=agent.agent_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def update_pool(env, pool,n_steps=100):\n",
    "    \"\"\" a function that creates new sessions and ads them into the pool\n",
    "    throwing the old ones away entirely for simplicity\"\"\"\n",
    "\n",
    "    preceding_memory_states = list(pool.prev_memory_states)\n",
    "    \n",
    "    #get interaction sessions\n",
    "    observation_tensor,action_tensor,reward_tensor,_,is_alive_tensor,_= pool.interact(step,n_steps=n_steps)\n",
    "        \n",
    "    #load them into experience replay environment\n",
    "    env.load_sessions(observation_tensor,action_tensor,reward_tensor,is_alive_tensor,preceding_memory_states)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#load first  sessions\n",
    "update_pool(env,pool,replay_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more sophisticated way of training is to store a large pool of sessions and train on random batches of them. \n",
    "* Why that is expected to be better - http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html\n",
    "* Or less proprietary - https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "\n",
    "To do that, one might make use of\n",
    "* ```env.load_sessions(...)``` - load new sessions\n",
    "* ```env.get_session_updates(...)``` - does the same thing via theano updates (advanced)\n",
    "* ```batch_env = env.sample_session_batch(batch_size, ...)``` - create an experience replay environment that contains batch_size random sessions from env (rerolled each time). Should be used in training instead of env.\n",
    "* ```env.select_session_batch(indices)``` does the same thing deterministically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with environment\n",
    "* An agent has a method that produces symbolic environment interaction sessions\n",
    "* Such sessions are in sequences of observations, agent memory, actions, q-values,etc\n",
    "  * one has to pre-define maximum session length.\n",
    "\n",
    "* SessionPool also stores rewards (Q-learning objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training via experience replay\n",
    "\n",
    "* We use agent we have created to replay environment interactions inside Theano\n",
    "* to than train on the replayed sessions via theano gradient propagation\n",
    "* this is essentially basic Lasagne code after the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#get agent's Qvalues obtained via experience replay\n",
    "_,_,_,_,qvalues_seq = agent.get_sessions(\n",
    "    env,\n",
    "    session_length=replay_seq_len,\n",
    "    batch_size=env.batch_size,\n",
    "    optimize_experience_replay=True,\n",
    ")\n",
    "\n",
    "\n",
    "#The \"_\"s are\n",
    "#first - environment states - which is empty since we are using session pool as our environment\n",
    "#secund - observation sequences - whatever agent recieved at observation input(s) on each tick\n",
    "#third - a dictionary of all agent memory units (RNN, GRU, NTM) - empty as we use none of them\n",
    "#last - \"imagined\" actions - actions agent would pick now if he was in that situation \n",
    "#                              - irrelevant since we are replaying and not actually playing the game now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating loss function\n",
    "* In this part we are using some basic Reinforcement Learning methods (here - Q-learning) to train\n",
    "* AgentNet has plenty of such methods, but we shall use the simple Q_learning for now.\n",
    "* Later you can try:\n",
    " * SARSA - simpler on-policy algorithms\n",
    " * N-step q-learning (requires n_steps parameter)\n",
    " * Advantage Actor-Critic (requires state values and probabilities instead of Q-values)\n",
    "\n",
    "\n",
    "* The basic interface is .get_elementwise_objective \n",
    "  * it returns loss function (here - squared error against reference Q-values) values at each batch and tick\n",
    "  \n",
    "* If you want to do it the hard way instead, try .get_reference_Qvalues and compute errors on ya own\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#get reference Qvalues according to Qlearning algorithm\n",
    "\n",
    "\n",
    "from agentnet.learning import qlearning\n",
    "\n",
    "#gamma - delayed reward coefficient - what fraction of reward is retained if it is obtained one tick later\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#IMPORTANT!\n",
    "# If you are training on a game that has rewards far outside some [-5,+5]\n",
    "# it is a good idea to downscale them to avoid divergence\n",
    "scaled_reward_seq = env.rewards\n",
    "#For SpaceInvaders, however, not scaling rewards is at least working\n",
    "\n",
    "\n",
    "elwise_mse_loss = qlearning.get_elementwise_objective(qvalues_seq,\n",
    "                                                        env.actions[0],\n",
    "                                                        scaled_reward_seq,\n",
    "                                                        env.is_alive,\n",
    "                                                        gamma_or_gammas=0.99,)\n",
    "\n",
    "#compute mean over \"alive\" fragments\n",
    "mse_loss = elwise_mse_loss.sum() / env.is_alive.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#regularize network weights\n",
    "\n",
    "from lasagne.regularization import regularize_network_params, l2\n",
    "reg_l2 = regularize_network_params(resolver,l2)*10**-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "loss = mse_loss + reg_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Compute weight updates\n",
    "updates = lasagne.updates.adadelta(loss,weights,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#mean session reward\n",
    "mean_session_reward = env.rewards.sum(axis=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile train and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "train_fun = theano.function([],[loss,mean_session_reward],updates=updates)\n",
    "evaluation_fun = theano.function([],[loss,mse_loss,reg_l2,mean_session_reward])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session visualization tools\n",
    "\n",
    "Just a helper function that draws current game images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def display_sessions(max_n_sessions = 3):\n",
    "    \"\"\"just draw random images\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=[15,8])\n",
    "    \n",
    "    pictures = [atari.render(\"rgb_array\") for atari in pool.games[:max_n_sessions]]\n",
    "    for i,pic in enumerate(pictures):\n",
    "        plt.subplot(1,max_n_sessions,i+1)\n",
    "        plt.imshow(pic)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#visualize untrained network performance (which is mostly random)\n",
    "display_sessions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from agentnet.display import Metrics\n",
    "score_log = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starting epoch\n",
    "epoch_counter = 1\n",
    "#moving average estimation\n",
    "alpha = 0.1\n",
    "ma_reward_current = 0.\n",
    "ma_reward_greedy = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n_epochs = 25000\n",
    "#25k may take hours to train.\n",
    "#consider interrupt early.\n",
    "\n",
    "\n",
    "\n",
    "for i in range(n_epochs):    \n",
    "    \n",
    "    \n",
    "    #train\n",
    "    update_pool(env,pool,replay_seq_len)\n",
    "    resolver.rng.seed(i)    \n",
    "    loss,avg_reward = train_fun()\n",
    "    \n",
    "    \n",
    "    ##update resolver's epsilon (chance of random action instead of optimal one)\n",
    "    if epoch_counter%1 ==0:\n",
    "        current_epsilon = 0.05 + 0.45*np.exp(-epoch_counter/1000.)\n",
    "        resolver.epsilon.set_value(np.float32(current_epsilon))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##record current learning progress and show learning curves\n",
    "    if epoch_counter%5 ==0:\n",
    "\n",
    "        ##update learning curves\n",
    "        full_loss, q_loss, l2_penalty, avg_reward_current = evaluation_fun()\n",
    "        ma_reward_current = (1-alpha)*ma_reward_current + alpha*avg_reward_current\n",
    "        score_log[\"expected e-greedy reward\"][epoch_counter] = ma_reward_current\n",
    "        \n",
    "        \n",
    "        \n",
    "        #greedy train\n",
    "        resolver.epsilon.set_value(0)\n",
    "        update_pool(env,pool,replay_seq_len)\n",
    "\n",
    "        avg_reward_greedy = evaluation_fun()[-1]\n",
    "        ma_reward_greedy = (1-alpha)*ma_reward_greedy + alpha*avg_reward_greedy\n",
    "        score_log[\"expected greedy reward\"][epoch_counter] = ma_reward_greedy\n",
    "        \n",
    "        \n",
    "        #back to epsilon-greedy\n",
    "        resolver.epsilon.set_value(np.float32(current_epsilon))\n",
    "        update_pool(env,pool,replay_seq_len)\n",
    "\n",
    "        print(\"epoch %i,loss %.5f, epsilon %.5f, rewards: ( e-greedy %.5f, greedy %.5f) \"%(\n",
    "            epoch_counter,full_loss,current_epsilon,ma_reward_current,ma_reward_greedy))\n",
    "        print(\"rec %.3f reg %.3f\"%(q_loss,l2_penalty))\n",
    "\n",
    "    if epoch_counter %500 ==0:\n",
    "        print(\"Learning curves:\")\n",
    "        score_log.plot()\n",
    "\n",
    "\n",
    "        print(\"Random session examples\")\n",
    "        display_sessions()\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "    epoch_counter  +=1\n",
    "\n",
    "    \n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating results\n",
    " * Here we plot learning curves and sample testimonials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "score_log.plot(\"final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Random session examples\")\n",
    "display_sessions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "Here we simply run the OpenAI gym submission code and view scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "resolver.epsilon.set_value(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AtariEnv' object has no attribute 'monitor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-cab128f6f3cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#this setup does\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0msubm_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mforce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\UserFiles\\anaconda\\envs\\ia\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"attempted to get missing private attribute '{}'\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'AtariEnv' object has no attribute 'monitor'"
     ]
    }
   ],
   "source": [
    "\n",
    "save_path = '/tmp/AgentNet-simplenet-SpaceInvadersv0-Recording0'\n",
    "\n",
    "subm_env = gym.make(GAME_TITLE)\n",
    "\n",
    "#starting monitor. This setup does not write videos\n",
    "#subm_env.monitor.start(save_path,lambda i: False,force=True)\n",
    "\n",
    "#this setup does\n",
    "subm_env.monitor.start(save_path,force=True)\n",
    "\n",
    "\n",
    "for i_episode in xrange(10):\n",
    "    \n",
    "    #initial observation\n",
    "    observation = subm_env.reset()\n",
    "    #initial memory\n",
    "    prev_memories = \"zeros\"\n",
    "    \n",
    "    \n",
    "    t = 0\n",
    "    while True:\n",
    "\n",
    "        action,new_memories = step([observation],prev_memories,batch_size=1)\n",
    "        observation, reward, done, info = subm_env.step(action[0])\n",
    "        \n",
    "        prev_memories = new_memories\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "        t+=1\n",
    "\n",
    "subm_env.monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.monitoring._monitors.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "gym.upload(save_path,\n",
    "           \n",
    "           #this notebook\n",
    "           writeup=<url to my gist>, \n",
    "           \n",
    "           #your api key\n",
    "           api_key=<my_own_api_key>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Once you got it working,\n",
    "Try building a network that maximizes the final score\n",
    "\n",
    "* Moar lasagne stuff: convolutional layers, batch normalization, nonlinearities and so on\n",
    "* Recurrent agent memory layers, GRUMemoryLayer, etc\n",
    "* Different reinforcement learning algorithm (p.e. qlearning_n_step), other parameters\n",
    "* Experience replay pool\n",
    "\n",
    "\n",
    "Look for examples? Try examples/Deep Kung Fu for most of these features\n",
    "\n",
    "\n",
    "You can also try to expand to a different game: \n",
    " * all OpenAI Atari games are already compatible, you only need to change GAME_TITLE\n",
    " * Other discrete action space environments are also accessible this way\n",
    " * For continuous action spaces, either discretize actions or use continuous RL algorithms (e.g. .learning.dpg_n_step)\n",
    " * Adapting to a custom non-OpenAI environment can be done with a simple wrapper\n",
    " \n",
    " \n",
    "__Good luck!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
