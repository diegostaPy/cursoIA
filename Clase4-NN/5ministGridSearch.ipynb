{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split, KFold\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.datasets import fetch_openml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\Anaconda\\envs\\IA\\lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# Cargar el conjunto de datos MNIST\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "\n",
    "# Normalizar las imágenes de 0 a 1\n",
    "X = X.astype('float32') / 255.0\n",
    "\n",
    "X=X.values\n",
    "# Convertir las etiquetas a enteros y luego a categóricas (one-hot encoding)\n",
    "y = to_categorical(y.astype('int'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer='adam', init='glorot_uniform', activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(784,)))\n",
    "    model.add(Dense(128, kernel_initializer=init, activation=activation))\n",
    "   # model.add(Dense(64, kernel_initializer=init, activation=activation))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_grid_search(X_train_k, y_train_k, X_val_k, y_val_k, param_grid):\n",
    "    for batch in param_grid['batch_size']:\n",
    "        model=create_model()\n",
    "        history=model.fit(X_train_k, y_train_k,validation_data=(X_val_k, y_val_k),batch_size=batch, epochs=2,verbose=1)\n",
    "\n",
    "    return batch,history.history['val_accuracy'][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1167/1167 [==============================] - 6s 4ms/step - loss: 0.3061 - accuracy: 0.9142 - val_loss: 0.1740 - val_accuracy: 0.9505\n",
      "Epoch 2/2\n",
      "1167/1167 [==============================] - 5s 4ms/step - loss: 0.1387 - accuracy: 0.9594 - val_loss: 0.1343 - val_accuracy: 0.9594\n",
      "Epoch 1/2\n",
      "584/584 [==============================] - 3s 4ms/step - loss: 0.3640 - accuracy: 0.8967 - val_loss: 0.2191 - val_accuracy: 0.9366\n",
      "Epoch 2/2\n",
      "584/584 [==============================] - 2s 4ms/step - loss: 0.1694 - accuracy: 0.9508 - val_loss: 0.1674 - val_accuracy: 0.9502\n",
      "Epoch 1/2\n",
      "292/292 [==============================] - 2s 4ms/step - loss: 0.4455 - accuracy: 0.8783 - val_loss: 0.2451 - val_accuracy: 0.9286\n",
      "Epoch 2/2\n",
      "292/292 [==============================] - 1s 3ms/step - loss: 0.2022 - accuracy: 0.9426 - val_loss: 0.1854 - val_accuracy: 0.9443\n",
      "0.9442867040634155\n",
      "Epoch 1/2\n",
      "1167/1167 [==============================] - 4s 4ms/step - loss: 0.3105 - accuracy: 0.9118 - val_loss: 0.1708 - val_accuracy: 0.9508\n",
      "Epoch 2/2\n",
      "1167/1167 [==============================] - 4s 4ms/step - loss: 0.1408 - accuracy: 0.9586 - val_loss: 0.1290 - val_accuracy: 0.9616\n",
      "Epoch 1/2\n",
      "584/584 [==============================] - 3s 4ms/step - loss: 0.3623 - accuracy: 0.8989 - val_loss: 0.2149 - val_accuracy: 0.9394\n",
      "Epoch 2/2\n",
      "584/584 [==============================] - 2s 4ms/step - loss: 0.1682 - accuracy: 0.9510 - val_loss: 0.1520 - val_accuracy: 0.9543\n",
      "Epoch 1/2\n",
      "292/292 [==============================] - 2s 4ms/step - loss: 0.4449 - accuracy: 0.8771 - val_loss: 0.2439 - val_accuracy: 0.9318\n",
      "Epoch 2/2\n",
      "292/292 [==============================] - 2s 5ms/step - loss: 0.2052 - accuracy: 0.9409 - val_loss: 0.1821 - val_accuracy: 0.9477\n",
      "0.9477152228355408\n",
      "Epoch 1/2\n",
      "1167/1167 [==============================] - 4s 4ms/step - loss: 0.3049 - accuracy: 0.9127 - val_loss: 0.1799 - val_accuracy: 0.9492\n",
      "Epoch 2/2\n",
      "1167/1167 [==============================] - 4s 4ms/step - loss: 0.1311 - accuracy: 0.9611 - val_loss: 0.1322 - val_accuracy: 0.9622\n",
      "Epoch 1/2\n",
      "584/584 [==============================] - 3s 4ms/step - loss: 0.3651 - accuracy: 0.8966 - val_loss: 0.2192 - val_accuracy: 0.9358\n",
      "Epoch 2/2\n",
      "584/584 [==============================] - 2s 4ms/step - loss: 0.1678 - accuracy: 0.9515 - val_loss: 0.1614 - val_accuracy: 0.9519\n",
      "Epoch 1/2\n",
      "292/292 [==============================] - 2s 5ms/step - loss: 0.4260 - accuracy: 0.8837 - val_loss: 0.2535 - val_accuracy: 0.9282\n",
      "Epoch 2/2\n",
      "292/292 [==============================] - 1s 4ms/step - loss: 0.1935 - accuracy: 0.9457 - val_loss: 0.1841 - val_accuracy: 0.9479\n",
      "0.9479267001152039\n",
      "\n",
      "Best Score: 0.9479 with Best Params: 128\n"
     ]
    }
   ],
   "source": [
    "# Define the grid of hyperparameters\n",
    "param_grid = {\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'epochs': [10],\n",
    "    'optimizer': ['adam'],\n",
    "    'init': ['normal'],\n",
    "    'activation': ['relu', 'tanh']\n",
    "}\n",
    "\n",
    "# Implement K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "best_params = None\n",
    "best_score = 0\n",
    "\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_train_k, X_val_k = X_train[train_index], X_train[val_index]\n",
    "    y_train_k, y_val_k = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    params, score = manual_grid_search(X_train_k, y_train_k, X_val_k, y_val_k, param_grid)\n",
    "    print(score)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_params = params\n",
    "\n",
    "print(f\"\\nBest Score: {best_score:.4f} with Best Params: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Score: 0.9479 with Best Params: 128\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nBest Score: {best_score:.4f} with Best Params: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the final model using the best hyperparameters\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m final_model \u001b[38;5;241m=\u001b[39m create_model(optimizer\u001b[38;5;241m=\u001b[39m\u001b[43mbest_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimizer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, \n\u001b[0;32m      3\u001b[0m                            init\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m      4\u001b[0m                            activation\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      6\u001b[0m final_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \n\u001b[0;32m      7\u001b[0m                 epochs\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m      8\u001b[0m                 batch_size\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m      9\u001b[0m                 verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Evaluate the final model on the test set\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Train the final model using the best hyperparameters\n",
    "final_model = create_model(optimizer=best_params['optimizer'], \n",
    "                           init=best_params['init'], \n",
    "                           activation=best_params['activation'])\n",
    "\n",
    "final_model.fit(X_train, y_train, \n",
    "                epochs=best_params['epochs'], \n",
    "                batch_size=best_params['batch_size'], \n",
    "                verbose=1)\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "test_score = final_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nFinal Test Accuracy: {test_score[1]:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
